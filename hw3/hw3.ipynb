{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 03 Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal \n",
    "\n",
    "The **goal** of this homework is to provide an opportunity to build an end-to-end system.\n",
    "\n",
    "Specifically, we are going to build a word embedding system, that can \n",
    "\n",
    "1. Read and preprocess raw data\n",
    "2. Use two different ways (latent semantic analysis and skip-gram) to learn word embeddings\n",
    "3. Evaluate the quality of word embeddings using some intrinsic evaluation methods \n",
    "\n",
    "### Submission\n",
    "\n",
    "Your submission should only include this notebook file. Please keep **all the outputs** of this notebook in your submission for grading. We will run the code only if we are not sure it is correct.\n",
    "\n",
    "### Dependency\n",
    "\n",
    "You will need the following package to finish this homework assignment\n",
    "\n",
    "- [nltk](https://www.nltk.org) for word tokenization\n",
    "- [fasttext](https://pypi.org/project/fasttext/) for the implementation of the Skip-gram model\n",
    "\n",
    "### Hint\n",
    "\n",
    "Search for the keyword `TODO` in this notebook to find out which parts need your input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparation\n",
    "\n",
    "Please run the following code to download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 10000 sentences\n"
     ]
    }
   ],
   "source": [
    "# Download the data from course webpage\n",
    "# import urllib.request\n",
    "# from os.path import isfile\n",
    "# if not isfile(\"embeddings/imdb-small.txt\"):\n",
    "#     url = \"https://yangfengji.net/uva-nlp-course/data/embeddings.zip\"\n",
    "#     print(\"Downloading ...\")\n",
    "#     filename, headers = urllib.request.urlretrieve(url, filename=\"embeddings.zip\")\n",
    "\n",
    "#     print(\"Decompressing the file ...\")\n",
    "#     !unzip embeddings.zip\n",
    "\n",
    "sents = open(\"embeddings/imdb-small.txt\").read().split(\"\\n\")\n",
    "print(\"Read {} sentences\".format(len(sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing is an **essential** skill for NLP researchers. Unlike machine learning where researchers sometimes may want to use synthetic data to demonstrate the potential of their algorithms, NLP researchers need to deal with real-world data all the time. Unfortunately, this means that these data are noisy and often contain irregular patterns. Therefore, a reasonable data processing can alleviate the challenge of building NLP systems to some extent and may also help boost the performance of machine learning models.\n",
    "\n",
    "Data processing for learning word embeddings includes two basic modules\n",
    "\n",
    "- Tokenizing texts and replacing some special tokens\n",
    "- Filtering low-frequency and building a vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tokenization (3 points)\n",
    "\n",
    "The following function *tokenize()* should include the following components\n",
    "\n",
    "1. Load the raw text from the file named **imdb-small.txt**\n",
    "2. Convert all characters into lower cases\n",
    "3. Tokenize the raw text using `nltk.tokenize` \n",
    "4. Remove all punctuation (as single tokens) and replace all numbers with a special token `<num>`\n",
    "5. Write the preprocessed text to the file named **imdb-small.txt.tokenized** and maintain the same format (one paragraph per line)\n",
    "\n",
    "(The file names are pre-defined, please do not change them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ft/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk import RegexpTokenizer\n",
    "\n",
    "def tokenize(infname=\"embeddings/imdb-small.txt\"):\n",
    "    # 1. Load the raw text from the file named imdb-small.txt\n",
    "    sentences = open(infname).read().split(\"\\n\")\n",
    "    outfname = open(infname + \".tokenized\", \"w\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    for s, sent in enumerate(sentences):\n",
    "        # 2. Convert all characters into lower cases\n",
    "        lower_case_sent = sent.lower()\n",
    "\n",
    "        # 3. Tokenize the raw text using nltk.tokenize\n",
    "        # 4. Remove all punctuation (as single tokens)\n",
    "        remove_punctuation = RegexpTokenizer(r\"\\w+\")\n",
    "        tokenized_sent = remove_punctuation.tokenize(lower_case_sent)\n",
    "\n",
    "        # 4. Replace all numbers with a special token <num>\n",
    "        for i, t in enumerate(tokenized_sent):\n",
    "            if t.isnumeric():\n",
    "                tokenized_sent[i] = \"<num>\"\n",
    "\n",
    "        # 5. Write the preprocessed text to the file\n",
    "        for t in tokenized_sent:\n",
    "            outfname.write('%s ' %t)\n",
    "        if s < 9999:\n",
    "            outfname.write('\\n')\n",
    "\n",
    "    # ----------------------------------------\n",
    "    outfname.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Filtering (2 points)\n",
    "\n",
    "The following function *token_filter()* should include the following components\n",
    "\n",
    "1. Remove the words that appear in the data less than 5 times (word_frequency < 5)\n",
    "2. Write the filtered data to the file named **imdb-small.txt.filtered** and maintain the same format (one sentence per line)\n",
    "3. Return a Python list that contains all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "\n",
    "def token_filter(infname=\"embeddings/imdb-small.txt.tokenized\", thresh=5):\n",
    "    tokenized_sentences = open(infname).read().split(\"\\n\")\n",
    "    outfname = open(infname.replace(\".tokenized\", \".filtered\"), 'w')\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    words = list()\n",
    "    for t_sent in tokenized_sentences:\n",
    "        for word in t_sent.split():\n",
    "            words.append(word)\n",
    "\n",
    "    # 1. Remove the words that appear in the data less than \"thresh\" times\n",
    "    vocab = Vocabulary(words, unk_cutoff=5)\n",
    "\n",
    "    # 2. Write the filtered data to the file named imdb-small.txt.filtered\n",
    "    for t, tokenized_sent in enumerate(tokenized_sentences):\n",
    "        filtered_sent = list()\n",
    "        for word in tokenized_sent.split():\n",
    "            # if vocab.lookup(word) != '<UNK>':\n",
    "            filtered_sent.append(vocab.lookup(word).lower())\n",
    "        \n",
    "        for word in filtered_sent:\n",
    "            outfname.write('%s ' %word)\n",
    "        if t < 9999:\n",
    "            outfname.write('\\n')\n",
    "\n",
    "    # ----------------------------------------\n",
    "    outfname.close()\n",
    "\n",
    "    # 3. Return a Python list that contains all the words\n",
    "    return list(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Put all together\n",
    "\n",
    "The following code block will call the previous two functions to do data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab size = 18323\n"
     ]
    }
   ],
   "source": [
    "tokenize()\n",
    "vocab = token_filter()\n",
    "print(\"The vocab size = {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Embeddings (5 points)\n",
    "\n",
    "In this section, you need to implement two different ways of constructing word embeddings: latent semantic analysis  and skipgram. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Latent semantic analysis (3 points)\n",
    "\n",
    "The function of LSA should include the following components\n",
    "\n",
    "- Construct the word-doc matrix using `CountVectorizer` with `tokenizer=lambda x : x.split()`, make sure in this matrix that each row represents one word and each column represents one document (sentence, to be accurate in this case)\n",
    "- Use the `TruncatedSVD` from `sklearn.decomposition` to factorize the word-doc matrix\n",
    "- Construct the word embedding matrix with dimensionality as $v \\times k$, where $v$ is the vocab size and $k$ is the word embedding dimension\n",
    "\n",
    "The LSA() function should return \n",
    "\n",
    "- **embeddings**: A matrix with size $v\\times k$ that contains all the word embeddings\n",
    "- **vocab**: A Python dict with size $v$ that maps a word to the corresponding word index. Please pay attention to the mapping relation in vocab, which will be needed in the evaluation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def LSA(fname = \"embeddings/imdb-small.txt.filtered\", dim=50):\n",
    "    sents = open(fname).read().split(\"\\n\")\n",
    "    \n",
    "    # 1. Construct the word-doc matrix using CountVectorizer\n",
    "    vectorizer =  CountVectorizer(tokenizer=lambda x : x.split())\n",
    "    word_doc = vectorizer.fit_transform(sents)\n",
    "    word_doc = word_doc.reshape(word_doc.shape[1], word_doc.shape[0])\n",
    "\n",
    "    # 2. Use TruncatedSVD to factorize the word-doc matrix\n",
    "    svd = TruncatedSVD(n_components=50)\n",
    "\n",
    "    # 3. Construct the word embedding matrix with dimensionality as v x k\n",
    "    embeddings = svd.fit_transform(word_doc)\n",
    "    \n",
    "    # Vocabulary\n",
    "    vocab = vectorizer.vocabulary_\n",
    "\n",
    "    return embeddings, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Skip-gram model (2 points)\n",
    "\n",
    "In this section, you do not have to implement the skip-gram model by yourself. An authentic implementation of skip-gram can be found in the Python package [fasttext](https://pypi.org/project/fasttext/), which you can install on the your local machine with the folllwing commandline or directly load the package if you are using Google Colab. \n",
    "```python\n",
    "pip install fasttext\n",
    "```\n",
    "\n",
    "In the following code, please use the `fasttext.train_unsupervised` function for the skipgram() implementation. For the `fasttext.train_unsupervised`, please use the following configurations\n",
    "\n",
    "- `model='skipgram'`\n",
    "- Context window size: `ws = 3` \n",
    "- Word embedding dimension: `dim = 50`\n",
    "- Number of negative examples: `neg = 5`\n",
    "\n",
    "For all other parameters, use their default values.\n",
    "\n",
    "Similar to the previous `LSA()`, `Skipgram()` should return \n",
    "\n",
    "- **embeddings**: A matrix with size $v\\times k$ that contains all the word embeddings\n",
    "- **vocab**: A Python dict with size $v$ that maps an index to the corresponding word\n",
    "\n",
    "To get the word embeddings and vocab from fasttext, you need to understand [some functions](https://pypi.org/project/fasttext/#api) provided by the `model` object in the fasttext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasttext import train_unsupervised\n",
    "\n",
    "def Skipgram(fname = \"embeddings/imdb-small.txt.filtered\", ws=3, dim=50):\n",
    "    model = train_unsupervised(\"embeddings/imdb-small.txt.filtered\", model='skipgram', ws=ws, dim=dim, neg=5)\n",
    "\n",
    "    embeddings = model.get_output_matrix()\n",
    "    \n",
    "    v = model.get_words()\n",
    "    vocab = dict()\n",
    "    for i, v_word in enumerate(v):\n",
    "        vocab[v_word] = i\n",
    "\n",
    "    return embeddings, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Put all together\n",
    "\n",
    "Run the following code blocks to get word embeddings from two different methods. It may take a couple of minutes to compute both embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_lsa, vocab_lsa = LSA()\n",
    "embeddings_sg, vocab_sg = Skipgram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18324"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will serve as the sanity check that `vocab_lsa` and `vocab_sg` contain the same words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word that only appear in one vocab: {'</s>'}\n"
     ]
    }
   ],
   "source": [
    "lsa_word_set = set([item[0] for item in vocab_lsa.items()])\n",
    "sg_word_set = set([item[0] for item in vocab_sg.items()])\n",
    "sym_diff = lsa_word_set.symmetric_difference(sg_word_set)\n",
    "\n",
    "if len(sym_diff) == 0:\n",
    "    print(\"vocab_lsa and vocab_sg contain the same words!\")\n",
    "else:\n",
    "    print(\"The word that only appear in one vocab: {}\".format(sym_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the only word from the `symmetric_difference()` function is `</s>`, then your implementation should be fine. (`</s>` was added by `fasttext` automatically to the end of each text.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation (6 points)\n",
    "\n",
    "In this homework, we will only use intrinsic evaluation. Specifically, for a list of predefined word pairs with their similarity scores, the evaluation is to calculate the correlation between the predefined similarity scores and the cosine similarity scores based on word embeddings. The higher the correlation, the better the quality of word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wordpairs(fname = \"embeddings/word-pairs.txt\", vocab=vocab):\n",
    "    records = {}\n",
    "    with open(fname) as fin:\n",
    "        for line in fin:\n",
    "            items = line.strip().split(\",\")\n",
    "            if (items[1] in vocab) and (items[2] in vocab): # make sure both words in the vocab\n",
    "                records[(items[1],items[2])] = float(items[3])\n",
    "    print(\"Load {} pairs of words for evaluation\".format(len(records)))\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Word similarity correlation (3 points)\n",
    "\n",
    "The purpose of this section is to implement the correlation function that compares the predefined scores and the scores computed by cosine similarity. The code of the correlation function is almost done, and the only thing left is the code for computing cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def correlation(records, embeddings, vocab):\n",
    "    predefined_scores = []\n",
    "    cossim_scores = []\n",
    "    for (words, sim_score) in records.items():\n",
    "        predefined_scores.append(sim_score)\n",
    "\n",
    "        word0 = embeddings[vocab[words[0]]]\n",
    "        word1 = embeddings[vocab[words[1]]]\n",
    "\n",
    "        cos_sim = cosine_similarity(word0.reshape(1,50),word1.reshape(1,50))\n",
    "        cossim_scores.append(cos_sim[0][0])\n",
    "\n",
    "    corr = pearsonr(predefined_scores, cossim_scores)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code block to calculate the correlations between pre-defined similarity scores and the cosine similarity scores based on word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 151 pairs of words for evaluation\n",
      "Load 151 pairs of words for evaluation\n"
     ]
    }
   ],
   "source": [
    "# Load records\n",
    "records_lsa = load_wordpairs(vocab=vocab_lsa)\n",
    "records_sg = load_wordpairs(vocab=vocab_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation with the LSA embeddings = 0.010522503294 with p-value 0.8979655774734477\n",
      "The correlation with the SG embeddings = 0.356725477141825 with p-value 6.927441041383524e-06\n",
      "Skipgram is better than LSA\n"
     ]
    }
   ],
   "source": [
    "corr_lsa = correlation(records_lsa, embeddings_lsa, vocab_lsa)\n",
    "print(\"The correlation with the LSA embeddings = {} with p-value {}\".format(corr_lsa[0], corr_lsa[1]))\n",
    "corr_sg = correlation(records_sg, embeddings_sg, vocab_sg)\n",
    "print(\"The correlation with the SG embeddings = {} with p-value {}\".format(corr_sg[0], corr_sg[1]))\n",
    "\n",
    "if corr_lsa[0] > corr_sg[0]:\n",
    "    print(\"LSA is better than Skip-gram\")\n",
    "elif corr_lsa[0] < corr_sg[0]:\n",
    "    print(\"Skipgram is better than LSA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analysis of context window size in Skipgram (3 points)\n",
    "\n",
    "With the correlation function, we can analyze the effect of different context window sizes in the Skipgram model. Specifically, please call the previous implementation \n",
    "\n",
    "- `Skipgram(fname, ws, dim=50)` with the context window size `ws` as 3, 6, 9, 12, 15\n",
    "- For each context window size, calculate the correlation using the function `correlation(records, embeddings, vocab)`\n",
    "- **Print out** the fives correlation scores in your final submission: one score per line with the following format\n",
    "<center> ws\\t correlation</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t0.22480088770813567\n",
      "6\t0.2684002356121604\n",
      "9\t0.2867693641700646\n",
      "12\t0.373773999400796\n",
      "15\t0.30800427112237727\n"
     ]
    }
   ],
   "source": [
    "for ws in [3,6,9,12,15]:\n",
    "    embeddings_sg, vocab_sg = Skipgram(ws=ws)\n",
    "    corr_sg = correlation(records_sg, embeddings_sg, vocab_sg)\n",
    "    print(\"{}\\t{}\".format(ws,corr_sg[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar experiment can also be conducted on the parameter of negative examples `neg`, but it will not be included in this homework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
